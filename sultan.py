import streamlit as st
import json
import arabic_reshaper
from bidi.algorithm import get_display
import networkx as nx
import matplotlib.pyplot as plt
import speech_recognition as sr
from PIL import Image
import matplotlib.font_manager as fm
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import gdown
# ØªØ­Ù…ÙŠÙ„ Ø®Ø· ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
arabic_font = fm.FontProperties(fname="arial.ttf")


def reshape_arabic_text(text):
    """Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ø±Ø³Ù…"""
    reshaped_text = arabic_reshaper.reshape(text)
    return get_display(reshaped_text)


@st.cache_data
def load_knowledge_base():
    """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ù…Ù„Ù JSON"""
    file_id = "1_Yi0K6hVGDJjZRhoxBJxrwyKLxbRPlCc"  # Replace with your file ID
    url = f"https://drive.google.com/uc?id={file_id}"
    output = "translated_diseases_v2.json"  # Output file name
    gdown.download(url, output, quiet=False, fuzzy=True)  # Use fuzzy to handle shareable links

    try:
        with open(output, "r", encoding="utf-8") as file:
            content = file.read()

            if not content:
                st.error("âš ï¸ Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙØ§Ø±Øº!")
                return {}
            data = json.loads(content)
            return data.get("symptoms", {})
    except FileNotFoundError:
        st.error("âš ï¸ Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        return {}
    except json.JSONDecodeError:
        st.error("âš ï¸ Ù…Ù„Ù Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ØºÙŠØ± ØµØ§Ù„Ø­!")
        return {}


def preprocess_text(text):
    """ØªÙ†Ø¸ÙŠÙ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„"""
    text = re.sub(r'[^\w\s]', '', text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©
    text = re.sub(r'[Ø£Ø¥Ø¢]', "Ø§", text)  # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø­Ø±ÙˆÙ
    text = re.sub(r'Ø©', "Ù‡", text)  # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø­Ø±ÙˆÙ
    return text.strip().split()


def build_semantic_network(knowledge_base):
    """Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø¯Ù„Ø§Ù„ÙŠØ© Ù„Ù„Ø£Ù…Ø±Ø§Ø¶ ÙˆØ§Ù„Ø£Ø¹Ø±Ø§Ø¶"""
    network = nx.DiGraph()
    for disease, details in knowledge_base.items():
        network.add_node(disease, type="disease")
        for symptom in details.get("Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶", []):
            network.add_node(symptom, type="symptom")
            network.add_edge(disease, symptom, relation="has_symptom")
    return network


def calculate_similarity(input_symptoms, disease_symptoms):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([input_symptoms, disease_symptoms])
    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
    return similarity[0][0]
def diagnose_disease(input_text, knowledge_base, semantic_network):
    """ØªØ´Ø®ÙŠØµ Ø§Ù„Ù…Ø±Ø¶ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ù…Ø¯Ø®Ù„Ø© Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"""
    input_symptoms = preprocess_text(input_text)
    disease_scores = []

    for disease in semantic_network.nodes:
        if semantic_network.nodes[disease].get("type") == "disease":
            disease_symptoms = [symptom for symptom in semantic_network.successors(disease)]

            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
            input_symptoms_str = " ".join(input_symptoms)
            disease_symptoms_str = " ".join(disease_symptoms)
            similarity = calculate_similarity(input_symptoms_str, disease_symptoms_str)

            # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©
            matched_symptoms = sum(1 for symptom in input_symptoms if symptom in disease_symptoms)
            total_symptoms = len(disease_symptoms)

            if matched_symptoms > 0:
                match_ratio = matched_symptoms / total_symptoms
                disease_scores.append((disease, match_ratio, matched_symptoms, similarity))

    if disease_scores:
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø«Ù… Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
        disease_scores.sort(key=lambda x: (-x[3], -x[1], -x[2]))
        best_match = disease_scores[0][0]
        details = knowledge_base.get(best_match, {})
        return best_match, details, disease_scores
    return None, None, []


def visualize_network(input_text, semantic_network):
    """Ø±Ø³Ù… Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© Ù„Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ù…Ø¯Ø®Ù„Ø©"""
    input_symptoms = preprocess_text(input_text)
    sub_network = nx.DiGraph()

    for symptom in input_symptoms:
        if semantic_network.has_node(symptom):
            sub_network.add_node(symptom, type="symptom")
            for disease in semantic_network.predecessors(symptom):
                sub_network.add_node(disease, type="disease")
                sub_network.add_edge(disease, symptom, relation="has_symptom")

    plt.figure(figsize=(10, 8))
    pos = nx.spring_layout(sub_network, seed=42)
    node_colors = ["lightblue" if sub_network.nodes[node].get("type") == "disease" else "lightcoral" for node in
                   sub_network.nodes]

    nx.draw(sub_network, pos, with_labels=True, node_color=node_colors, node_size=1500, font_size=10,
            font_family="Arial")
    st.pyplot(plt)

def main():
    st.title("ğŸ”¬ Ù†Ø¸Ø§Ù… ØªØ´Ø®ÙŠØµ Ø·Ø¨ÙŠ Ø°ÙƒÙŠ")

    knowledge_base = load_knowledge_base()
    if not knowledge_base:
        return

    semantic_network = build_semantic_network(knowledge_base)

    image = Image.open("images (2).jpg")
    st.image(image, use_container_width=True)

    # Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ ÙŠØ¯ÙˆÙŠÙ‹Ø§
    input_text = st.text_input("Ø£Ø¯Ø®Ù„ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶:")

    # Ø²Ø± Ø§Ù„ØªØ´Ø®ÙŠØµ
    if st.button("ØªØ´Ø®ÙŠØµ"):
        if input_text:
            disease, details, ranked_diseases = diagnose_disease(input_text, knowledge_base, semantic_network)
            if disease:
                st.success(f"ğŸ¦  Ø§Ù„Ù…Ø±Ø¶ Ø§Ù„Ù…Ø­ØªÙ…Ù„: {disease}")
                st.write(f"ğŸ“– Ø§Ù„ÙˆØµÙ: {details.get('Ø§Ù„ÙˆØµÙ', '')}")
                st.write(f"ğŸ’Š Ø§Ù„Ø¹Ù„Ø§Ø¬: {', '.join(details.get('Ø§Ù„Ø¹Ù„Ø§Ø¬', []))}")
            else:
                st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø±Ø¶ Ù…Ø·Ø§Ø¨Ù‚.")
    # Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø±Ø¶ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹ÙƒØ³ÙŠ
    reverse_search = st.text_input("ğŸ” Ø£Ø¯Ø®Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø±Ø¶ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø¹Ø±Ø§Ø¶Ù‡:")

    if st.button("ğŸ”„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹ÙƒØ³ÙŠ"):
        if reverse_search:
            disease_name = reverse_search.strip()
            if disease_name in knowledge_base:
                details = knowledge_base[disease_name]
                symptoms = details.get("Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶", [])
                st.success(f"âœ… Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ù€ {disease_name}:")
                st.write(f"ğŸ“ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶: {', '.join(symptoms)}")
            else:
                st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø±Ø¶ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        else:
            st.warning("âŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø±Ø¶ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù†Ù‡.")

    # Ø²Ø± Ø¹Ø±Ø¶ Ø§Ù„Ø´Ø¨ÙƒØ©
    if st.button("ğŸ“ŠØ¹Ø±Ø¶ Ø§Ù„Ø´Ø¨ÙƒØ©"):
        if input_text:
            visualize_network(input_text, semantic_network)
        else:
            st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø£ÙˆÙ„Ø§Ù‹.")


if __name__ == "__main__":
    main()
